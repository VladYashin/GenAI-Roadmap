{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Model Parameters\n",
    "\n",
    "This document provides an overview of various parameters that can be adjusted when interacting with OpenAI models to control and refine their outputs.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Temperature](#temperature)\n",
    "2. [Top Probabilities (Top P)](#top-probabilities-top-p)\n",
    "3. [Max Length (tokens)](#max-length-tokens)\n",
    "4. [Stop Sequences](#stop-sequences)\n",
    "5. [Frequency Penalty](#frequency-penalty)\n",
    "6. [Presence Penalty](#presence-penalty)\n",
    "7. [Pre-response Text](#pre-response-text)\n",
    "8. [Post-response Text](#post-response-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules & configuration\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "dotenv_path = os.path.join(os.path.dirname(os.getcwd()), '.env')  # Assumes .env is in the parent directory of your notebook\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Access environment variables\n",
    "AZURE_OPENAI_API_KEY = os.environ.get('AZURE_OPENAI_KEY')\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_VERSION = os.environ.get('AZURE_OPENAI_VERSION')\n",
    "\n",
    "# Set OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = AZURE_OPENAI_API_KEY\n",
    "openai.api_base = AZURE_OPENAI_ENDPOINT\n",
    "openai.api_version = AZURE_OPENAI_VERSION\n",
    "\n",
    "# Setting constant for text-davinci-003 model used, name of deployment in azure resource\n",
    "deployment_name = \"text-davinci-003\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "Temperature plays a crucial role in controlling the randomness & diversity of model outputs:\n",
    "\n",
    "- **High Values (e.g., 0.7):** Produce more diverse and creative outputs.\n",
    "- **Low Values (e.g., 0.2):** Yield more deterministic and focused results.\n",
    "  \n",
    "At the core of its operation, temperature alters the probability distribution over potential tokens during text generation. A temperature of 0 renders the model entirely deterministic, always opting for the most probable token.\n",
    "\n",
    "> **<font color=\"red\">NOTE:</font>** It's recommended to adjust either Temperature or Top P, but not both.\n",
    "\n",
    "### Example:\n",
    "With a temperature of 0.2, the response might be more focused and deterministic. For instance, when asking for a color of the sky, the model is likely to answer \"blue\" more consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m temperatures \u001b[39m=\u001b[39m [\u001b[39m0.0\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m0.3\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.7\u001b[39m, \u001b[39m0.9\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m idx, temp \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(temperatures, \u001b[39m1\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      8\u001b[0m         engine\u001b[39m=\u001b[39;49mdeployment_name,\n\u001b[0;32m      9\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt_i,\n\u001b[0;32m     10\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemp,\n\u001b[0;32m     11\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m     12\u001b[0m         top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     13\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[0;32m     14\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m\n\u001b[0;32m     15\u001b[0m     )\n\u001b[0;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExample \u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTemperature: \u001b[39m\u001b[39m{\u001b[39;00mtemp\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\GenAI_roadmap\\.venv\\Lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\GenAI_roadmap\\.venv\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[0;32m    141\u001b[0m         timeout,\n\u001b[0;32m    142\u001b[0m         stream,\n\u001b[0;32m    143\u001b[0m         headers,\n\u001b[0;32m    144\u001b[0m         request_timeout,\n\u001b[0;32m    145\u001b[0m         typed_api_type,\n\u001b[0;32m    146\u001b[0m         requestor,\n\u001b[0;32m    147\u001b[0m         url,\n\u001b[0;32m    148\u001b[0m         params,\n\u001b[1;32m--> 149\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__prepare_create_request(\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[0;32m    153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GenAI_roadmap\\.venv\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:106\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[1;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    104\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m MAX_TIMEOUT\n\u001b[1;32m--> 106\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39;49mAPIRequestor(\n\u001b[0;32m    107\u001b[0m     api_key,\n\u001b[0;32m    108\u001b[0m     api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[0;32m    109\u001b[0m     api_type\u001b[39m=\u001b[39;49mapi_type,\n\u001b[0;32m    110\u001b[0m     api_version\u001b[39m=\u001b[39;49mapi_version,\n\u001b[0;32m    111\u001b[0m     organization\u001b[39m=\u001b[39;49morganization,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    113\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    115\u001b[0m     deployment_id,\n\u001b[0;32m    116\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     params,\n\u001b[0;32m    125\u001b[0m )\n",
      "File \u001b[1;32mc:\\GenAI_roadmap\\.venv\\Lib\\site-packages\\openai\\api_requestor.py:138\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[1;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    131\u001b[0m     key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     organization\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    136\u001b[0m ):\n\u001b[0;32m    137\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_base \u001b[39m=\u001b[39m api_base \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_base\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m key \u001b[39mor\u001b[39;00m util\u001b[39m.\u001b[39;49mdefault_api_key()\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_type \u001b[39m=\u001b[39m (\n\u001b[0;32m    140\u001b[0m         ApiType\u001b[39m.\u001b[39mfrom_str(api_type)\n\u001b[0;32m    141\u001b[0m         \u001b[39mif\u001b[39;00m api_type\n\u001b[0;32m    142\u001b[0m         \u001b[39melse\u001b[39;00m ApiType\u001b[39m.\u001b[39mfrom_str(openai\u001b[39m.\u001b[39mapi_type)\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_version \u001b[39m=\u001b[39m api_version \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_version\n",
      "File \u001b[1;32mc:\\GenAI_roadmap\\.venv\\Lib\\site-packages\\openai\\util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m openai\u001b[39m.\u001b[39mapi_key\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAuthenticationError(\n\u001b[0;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo API key provided. You can set your API key in code using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key = <API-KEY>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key_path = <PATH>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details."
     ]
    }
   ],
   "source": [
    "prompt_i = \"What is sky?\"\n",
    "\n",
    "# List of temperatures to test\n",
    "temperatures = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "for idx, temp in enumerate(temperatures, 1):\n",
    "    response = openai.Completion.create(\n",
    "        engine=deployment_name,\n",
    "        prompt=prompt_i,\n",
    "        temperature=temp,\n",
    "        max_tokens=50,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    print(f\"Example {idx}:\")\n",
    "    print(f\"Temperature: {temp}\\n\")\n",
    "    print(response.choices[0].text.strip())  # Removing extra whitespace\n",
    "    print('\\n' + '-' * 50 + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Top Probabilities (Top P)\n",
    "Top-p sampling is a method where the model dynamically selects words based on a cumulative probability threshold, p. Instead of always picking the most probable word or considering a fixed number of top words, Top-p sampling evaluates the cumulative distribution up to the set threshold. For instance, with top_p set to 0.8, the model considers only the top 20% probability mass of the next word's possible options.\n",
    "\n",
    "- **Lower Values:** The model narrows its token selection to those more likely.\n",
    "- **Higher Values:** The model considers a broader range of tokens, both high and low likelihood.\n",
    "- Instead of evaluating all potential tokens, only a subset (referred to as the nucleus) is considered.\n",
    "- This subset is determined by the cumulative probability mass meeting a certain threshold defined by top_p.\n",
    "\n",
    "For instance, with top_p set to 0.1, GPT-3 will only contemplate the top 10% tokens by probability mass for generating the next token, enabling dynamic vocabulary selection based on the context.\n",
    "\n",
    "> **<font color=\"red\">NOTE:</font>** It is recommended to adjust either temperature or Top P, but NOT both.\n",
    "\n",
    "### Example:\n",
    "A Top P value of 0.8 might result in slightly diverse outputs for the same prompt over multiple attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      " The only sound was the occasional rustle of leaves in the wind and the distant hoot of an owl. The moonlight shone through the trees, casting eerie shadows across the forest floor. The air was still and heavy, and the atmosphere was tense. There was a feeling of dread in the air, as if something was lurking in the shadows, waiting to pounce.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      " The only sound that could be heard was the faint chirping of crickets and the rustling of the trees in the wind. The darkness of the night seemed to swallow up the light from the stars, making it almost impossible to see. Despite the darkness, the silhouette of the trees could be seen against the moonlight, creating an eerie atmosphere. There was an air of anticipation as if something was about to happen, but nothing ever did. The only thing that remained was the silence and the\n"
     ]
    }
   ],
   "source": [
    "prompt_i = \"The forest was eerily quiet that night.\"\n",
    "print(\"Example 1:\")\n",
    "response = openai.Completion.create(\n",
    "    engine=deployment_name,\n",
    "    prompt=prompt_i,\n",
    "    top_p=0.3,\n",
    "    max_tokens=200\n",
    ")\n",
    "print(response.choices[0].text)\n",
    "\n",
    "print('\\n' + '-' * 50 + '\\n')\n",
    "\n",
    "prompt_i = \"The forest was eerily quiet that night.\"\n",
    "print(\"Example 2:\")\n",
    "response = openai.Completion.create(\n",
    "    engine=deployment_name,\n",
    "    prompt=prompt_i,\n",
    "    top_p=0.7,\n",
    "    max_tokens=200\n",
    ")\n",
    "print(response.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Using Temperature and Top P\n",
    "\n",
    "Both parameters can be utilized individually or in tandem when interfacing with the API. Adjusting them helps tailor model's outputs to a diverse array of applications.\n",
    "\n",
    "Below is a table highlighting typical use cases and suggested parameter values:\n",
    "\n",
    "| Use Case                  | Temperature | Top P | Description                                                                                      |\n",
    "|---------------------------|-------------|-------|--------------------------------------------------------------------------------------------------|\n",
    "| Code Generation           | 0.2         | 0.1   | Adheres to established code patterns and conventions. Outputs deterministic, focused code.       |\n",
    "| Creative Writing          | 0.7         | 0.8   | Yields creative, diverse text ideal for storytelling. Explores beyond typical patterns.          |\n",
    "| Chatbot Responses         | 0.5         | 0.5   | Balances coherence with diversity. Creates natural, engaging conversation.                       |\n",
    "| Code Comment Generation   | 0.3         | 0.2   | Produces concise, relevant code comments. Adheres to conventions.                               |\n",
    "| Data Analysis Scripting   | 0.2         | 0.1   | Generates correct, efficient scripts for data analysis. Emphasizes determinism and focus.        |\n",
    "| Exploratory Code Writing  | 0.6         | 0.7   | Creates code that ventures into alternative solutions and creative approaches.                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Length (tokens)\n",
    "\n",
    "Defines the upper limit for tokens per one model response:\n",
    "- **Maximum Tokens:** The API supports up to 4000 tokens, shared between prompts and model's response.\n",
    "- **Token Approximation:** Typically, 1 token is roughly equivalent to 3.5-4 characters in English.\n",
    "\n",
    "### Example:\n",
    "Setting the max length to 50 tokens will restrict the response to approximately 175-200 characters. \n",
    "\n",
    "To learn & recap what tokens are visit our [tokenization]() folder \n",
    "\n",
    "ToDo: Add link to tokenization folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m max_length \u001b[39m=\u001b[39m [\u001b[39m30\u001b[39m, \u001b[39m200\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m idx, mtoken \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(max_length, \u001b[39m1\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      8\u001b[0m         engine\u001b[39m=\u001b[39;49mdeployment_name,\n\u001b[0;32m      9\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt_i,\n\u001b[0;32m     10\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmtoken,\n\u001b[0;32m     12\u001b[0m         top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     13\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[0;32m     14\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m\n\u001b[0;32m     15\u001b[0m     )\n\u001b[0;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExample \u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMax token: \u001b[39m\u001b[39m{\u001b[39;00mmtoken\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\GenAI_roadmap\\.venv\\Lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\GenAI_roadmap\\.venv\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[0;32m    141\u001b[0m         timeout,\n\u001b[0;32m    142\u001b[0m         stream,\n\u001b[0;32m    143\u001b[0m         headers,\n\u001b[0;32m    144\u001b[0m         request_timeout,\n\u001b[0;32m    145\u001b[0m         typed_api_type,\n\u001b[0;32m    146\u001b[0m         requestor,\n\u001b[0;32m    147\u001b[0m         url,\n\u001b[0;32m    148\u001b[0m         params,\n\u001b[1;32m--> 149\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__prepare_create_request(\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[0;32m    153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GenAI_roadmap\\.venv\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:106\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[1;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    104\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m MAX_TIMEOUT\n\u001b[1;32m--> 106\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39;49mAPIRequestor(\n\u001b[0;32m    107\u001b[0m     api_key,\n\u001b[0;32m    108\u001b[0m     api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[0;32m    109\u001b[0m     api_type\u001b[39m=\u001b[39;49mapi_type,\n\u001b[0;32m    110\u001b[0m     api_version\u001b[39m=\u001b[39;49mapi_version,\n\u001b[0;32m    111\u001b[0m     organization\u001b[39m=\u001b[39;49morganization,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    113\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    115\u001b[0m     deployment_id,\n\u001b[0;32m    116\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     params,\n\u001b[0;32m    125\u001b[0m )\n",
      "File \u001b[1;32mc:\\GenAI_roadmap\\.venv\\Lib\\site-packages\\openai\\api_requestor.py:138\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[1;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    131\u001b[0m     key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     organization\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    136\u001b[0m ):\n\u001b[0;32m    137\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_base \u001b[39m=\u001b[39m api_base \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_base\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m key \u001b[39mor\u001b[39;00m util\u001b[39m.\u001b[39;49mdefault_api_key()\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_type \u001b[39m=\u001b[39m (\n\u001b[0;32m    140\u001b[0m         ApiType\u001b[39m.\u001b[39mfrom_str(api_type)\n\u001b[0;32m    141\u001b[0m         \u001b[39mif\u001b[39;00m api_type\n\u001b[0;32m    142\u001b[0m         \u001b[39melse\u001b[39;00m ApiType\u001b[39m.\u001b[39mfrom_str(openai\u001b[39m.\u001b[39mapi_type)\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_version \u001b[39m=\u001b[39m api_version \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_version\n",
      "File \u001b[1;32mc:\\GenAI_roadmap\\.venv\\Lib\\site-packages\\openai\\util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m openai\u001b[39m.\u001b[39mapi_key\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAuthenticationError(\n\u001b[0;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo API key provided. You can set your API key in code using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key = <API-KEY>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key_path = <PATH>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details."
     ]
    }
   ],
   "source": [
    "prompt_i = \"What is marketing?\"\n",
    "\n",
    "# List of temperatures to test\n",
    "max_length = [30, 200]\n",
    "\n",
    "for idx, mtoken in enumerate(max_length, 1):\n",
    "    response = openai.Completion.create(\n",
    "        engine=deployment_name,\n",
    "        prompt=prompt_i,\n",
    "        temperature=0.1,\n",
    "        max_tokens=mtoken,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    print(f\"Example {idx}:\")\n",
    "    print(f\"Max token: {mtoken}\\n\")\n",
    "    print(response.choices[0].text.strip())  # Removing extra whitespace\n",
    "    print('\\n' + '-' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Sequences\n",
    "Allows the model to halt responses at a specified point:\n",
    "- **Sequences:** Up to four sequences can be defined for the model to stop.\n",
    "- **Output:** The returned response will not include the defined stop sequence.\n",
    "\n",
    "### Example:\n",
    "Given the stop sequence as \".\", the model will stop generating tokens right after completing a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Sequences Example:\n",
      " Coffee?\n"
     ]
    }
   ],
   "source": [
    "prompt_i = '''\n",
    "Rewrite the given sentence in a shorter form.\n",
    "\n",
    "Sentence: Do you want to go for a coffee\n",
    "Rewrite:'''\n",
    "response = openai.Completion.create(\n",
    "    engine=deployment_name,\n",
    "    prompt=prompt_i,\n",
    "    max_tokens=50,\n",
    "    stop=[\"Sentence\"]  # Stop as soon as either \"apple\" or \"banana\" is generated\n",
    ")\n",
    "print(\"Stop Sequences Example:\")\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Penalty\n",
    "Reduces the probability of token repetition based on its recurrence in the generated text.\n",
    "\n",
    "### Example:\n",
    "Given a high frequency penalty, a response to \"List fruits\" might be less likely to mention \"apple\" multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Penalty Example:\n",
      "\n",
      "\n",
      "Glorious, magical, breathtaking, dazzling, vibrant, shimmering, serene, peaceful, alluring, romantic.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_i = \"Describe the sunrise in the mountains using adjectives:\"\n",
    "response = openai.Completion.create(\n",
    "    engine=deployment_name,\n",
    "    prompt=prompt_i,\n",
    "    max_tokens=200,\n",
    "    frequency_penalty=-0.5  # Penalize tokens that appear frequently\n",
    ")\n",
    "print(\"Frequency Penalty Example:\")\n",
    "print(response.choices[0].text)\n",
    "print('\\n' + '-' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presence Penalty\n",
    "Minimizes the likelihood of reusing any token present in the text, promoting the introduction of new topics.\n",
    "\n",
    "### Example:\n",
    "With an adjusted presence penalty, the model might diversify topics when asked to \"Discuss various subjects.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence Penalty Example:\n",
      "\n",
      "\n",
      "Football: Football is probably the most popular and widely watched sport in the world. It’s a team game that requires strategy, athleticism, and athleticism. Football is a game that is often compared to a chess match because of the strategies and skill that is required to be successful.\n",
      "\n",
      "Basketball: Basketball is a sport that is known for its creative and intense nature. It is a fast-paced, action-packed game that requires teamwork, strategy, and athleticism. It is\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_i = \"Talk about various sports:\"\n",
    "response = openai.Completion.create(\n",
    "    engine=deployment_name,\n",
    "    prompt=prompt_i,\n",
    "    max_tokens=200,\n",
    "    presence_penalty=-0.5  # Penalize tokens that have already appeared\n",
    ")\n",
    "print(\"Presence Penalty Example:\")\n",
    "print(response.choices[0].text)\n",
    "print('\\n' + '-' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-response Text\n",
    "Allows the insertion of text post user's input but prior to the model's answer, prepping the model for its response.\n",
    "\n",
    "### Example:\n",
    "Given the pre-response text \"Recall historical events,\" when the user queries \"Tell me about Rome,\" the model might lean towards ancient Roman history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-response Text Example:\n",
      "prompt_i:  Translate the following English sentence to French: 'TRANSLATION: Hello, how are you?'\n",
      "\n",
      "\n",
      "Bonjour, comment allez-vous ?\n"
     ]
    }
   ],
   "source": [
    "prompt_i = \"Translate the following English sentence to French: '{pre_response}Hello, how are you?'\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=deployment_name,\n",
    "    prompt=prompt_i.format(pre_response=\"TRANSLATION: \"),\n",
    ")\n",
    "print(\"Pre-response Text Example:\")\n",
    "print('prompt_i: ', prompt_i.format(pre_response=\"TRANSLATION: \"))\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Post-response Text\n",
    "Inserts text succeeding the model's generated output, prompting further user engagement, especially in conversational contexts.\n",
    "\n",
    "### Example:\n",
    "A post-response text like \"What are your thoughts on that?\" can turn the model's answer into a conversation starter, encouraging users to continue the dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-response Text Example:\n",
      "\n",
      "\n",
      "The capital of France is Paris. (For more information, visit a geography website!)\n"
     ]
    }
   ],
   "source": [
    "prompt_i = \"Do you know what's the highest mountain in the USA?\"\n",
    "response = openai.Completion.create(\n",
    "    engine=deployment_name,\n",
    "    prompt=prompt_i,\n",
    ")\n",
    "post_response = \" (For more information, please, visit our website!)\"\n",
    "print(\"Post-response Text Example:\")\n",
    "print(response.choices[0].text + post_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
